SVM

SVM이란?
머신러닝의 지도학습 기법으로 주로 분류에 자주 사용되는 SVM에 대하여 알아보고자 한다.
SupportVectorMachine의 줄일말로 SupprotVector(서포트 벡터)와 Hyperplane(초평면)을 이용해
DecisionBoundary(결정 경계)를 정의하고 분류를 시행하는 알고리즘이다.


>DecisionBoundry
두 개의 계층을 가지고 있는 통계적인 분류 문제에서,
결정 경계는 기본 벡터공간을 각 클래스에 대하여 하나씩 두 개의 집합으로 나누는 초표면이다.
분류기는 결정 경계의 한쪽에 있는 모든 점을 한 클래스,
다른 한쪽에 있는 모든 점을 다른 클래스에 속하는 것으로 분류한다.
결정 경계는 출력 계층이 모호한 문제 공간의 영역이다.
만약 결정표면이 초평면이라면, 분류문제는 선형이며 그 계층들은 선형적으로 분리가능하다.
결정 경계가 항상 명확한 것은 아니다.
즉 특징 공간안에서의 한 계층으로부터 다른 계층으로의 전이는 불연속적인 게 아니라 점진적이다.
이 효과는 계층들이 모호할 때 퍼지 논리 기반의 분류 알고리즘에서 보편적이다.


SVM은 최적의 결정 경계를 찾는 것이 목적인데 여기서 최적의 결정 경계를 찾기 위해서는
Margin(마진)의 최대점을 찾아야한다. 마진은 결정 경계와 서포트 벡터 사이의 거리로 큰 값을 가질수록
분류 경계가 커짐을 뜻하기 때문에 좋은 분류 성능을 보여줄것이다.

-SVM이미지

>SVM 구성요소
Hyperplane : 초평면으로 고차원의 결정 경계를 뜻하기도 한다.
SupportVector : 결정 경계와 가까이 있는 데이터 포인터
margin : 결정 경계와 서포트 벡터 사이의 거리


SVM 선형분리
SVM 비선형분리

SVM에서 결정 경계는 p차원의 데이터라면 p-1차원의 결정 경계가 만들어진다.
예를 들어, 3차원의 데이터는 2차원의 평면으로 경계가 만들어지고,
2차원의 데이터는 1차원의 벡터(선)으로 경계를 만든다.
이러한 결정 경계의 특징 때문에 초기에 결정 경계를 구하는 알고리즘은 선형 분류만 가능했었고,
비선형 분류가 불가능했었다. 하지만 데이터의 분포에 커널 트릭을 적용시켜 분포를 고차원으로 변형함으로써
비선형 분류도 가능하도록 만들었다.

커널트릭
저차원 공간을 고차원으로 매핑해주는 작업,
선형 분리가 불가능한 1차원 데이터를 2차원의 공간으로 매핑하여 선형분리가 가능하도록 한다.

-커널트릭이미지

위의 그림은 커널트릭을 활용해 선형분리가 불가능한 1차원의 데이터에
2차원의 데이터를 추가함으로써 선형분리가 가능하도록 하는것이다.


그렇다면 커널트릭을 적용해 저차원의 데이터를 고차원으로 매핑하여 시행하는 분류의 성능은
선형분류보다 정확한 성능을 보여주는 것인가? 하는 의문이 생긴다. 2차원의 데이터를 1차원의
벡터로 선형 분류를 시행하는 방법과 2차원의 데이터를 3차원으로 변형시켜 2차원의 초평면으로
비선형 분류를 시행하는 방법중 어떤 것이 더 높은 정확도를 보여주는지 파악해보았다.

-분류 코드
겹쳐져있는 데이터, 겹쳐지지 않은 데이터
2차원 1차원으로 분류, 2차원 3차원 변형 후 분류


분류를 시행해 본 결과, 고차원 공간으로 변형시킨 방법은
다시 본래의 차원에서 과적합을 초래할수있다는 것이다.

고차원 공간에서의 분류는 기존 공간에서의 분류 일반화의 오차를 상승시킨다는 것이 증명되었고,
그 오차의 상승 폭에 대한 임계값이 있다.
과적합까지의 임계값이 있다.


SVM에서 중요한 하이퍼 파라미터로는
kernel(kernel type) -
C(규제) - 클수록 강력한 규제, 클수록 과적합
gamma(커널 계수, 가우시안 커널 폭을 제어, 데이터 포인트 사이의 거리)
클수록 데이터 포인트 사이의 거리가 , 클수록 과적합














비선형분리

Hyperplane
p차원에서 p-1차원의 Hyperplane 생성

분류기는 고차원 특징 공간에서 선형 초평면이지만
기존 차원 공간에서는 비선형 초평면이다
초평면의 차원증가???
고차원 공간을 이용한 분류는 과적합을 초래하여
기존 공간에서 분류 일반화의 오차를 상승시킨다.
				(기존 공간에서 분류 오차를 증가시킨다.)
				(기존 공간에서의 오분류를 발생시킨다.)









선형분리
p개의 변수, p차원을 p-1 차원으로 분리
3차원을 2차원으로 분리, 2차원을 1차원으로 분리


소프트벡터머신에 가장 근접한 값들만 활용해 서포트벡터를 그린다.


n개의 속성을 가진 데이터는 최소 n+1개의 서포트 벡터가 존재한다.












from sklearn.datasets.samples_generator import make_blobs
X, y = make_blobs(n_samples=40, centers=2, random_state=20)



option
	kernel
	C
	gamma


svm = SVC(kernel='rbf', random_state=1, gamma=0.10, C=10.0)
svm.fit(X_xor, y_xor)
plot_decision_regions(X_xor, y_xor, classifier=svm)
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()

clf = svm.SVC(kernel='linear')
clf.fit(X_train, y_train)
plot_decision_function(X_train, y_train, X_test, y_test, clf)



하이퍼 파라미터별로 그래프 그려본 후
그리드 서치를 통한 최적의 하이퍼파라미터 설정


파라미터확인
class_weight_ : ndarray of shape (n_classes,)
classes_ : ndarray of shape (n_classes,)
coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)
dual_coef_ : ndarray of shape (n_classes -1, n_SV)
fit_status_ : int
intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)
support_ : ndarray of shape (n_SV)
support_vectors_ : ndarray of shape (n_SV, n_features)   #서포트벡터 위치 반환 scatter활용
n_support_ : ndarray of shape (n_classes,), dtype=int32  #서포트벡터 갯수


커널 활용한 데이터 선형 분리 시각화


결정경계 시각화
import mglearn

plt.figure(figsize=[10,8])
mglearn.plots.plot_2d_classification(model, X_train, cm='spring')
mglearn.discrete_scatter(X_train[:,0], X_train[:,1], y_train)
